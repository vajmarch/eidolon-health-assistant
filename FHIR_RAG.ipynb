{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcbc79e952c4890",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Some constants to use throughout \n",
    "\n",
    "in_file_glob = './working/data/*.json'\n",
    "flat_file_path = './working/flat'\n",
    "vector_store_file_path = './working/vector_store'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6665eba975cf558",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "camel_pattern1 = re.compile(r'(.)([A-Z][a-z]+)')\n",
    "camel_pattern2 = re.compile(r'([a-z0-9])([A-Z])')\n",
    "\n",
    "\n",
    "def split_camel(text):\n",
    "    new_text = camel_pattern1.sub(r'\\1 \\2', text)\n",
    "    new_text = camel_pattern2.sub(r'\\1 \\2', new_text)\n",
    "    return new_text\n",
    "\n",
    "\n",
    "def handle_special_attributes(attrib_name, value):\n",
    "    if attrib_name == 'resource Type':\n",
    "        return split_camel(value)\n",
    "    return value\n",
    "\n",
    "\n",
    "def flatten_fhir(nested_json):\n",
    "    out = {}\n",
    "\n",
    "    def flatten(json_to_flatten, name=''):\n",
    "        if type(json_to_flatten) is dict:\n",
    "            for sub_attribute in json_to_flatten:\n",
    "                flatten(json_to_flatten[sub_attribute], name + split_camel(sub_attribute) + ' ')\n",
    "        elif type(json_to_flatten) is list:\n",
    "            for i, sub_json in enumerate(json_to_flatten):\n",
    "                flatten(sub_json, name + str(i) + ' ')\n",
    "        else:\n",
    "            attrib_name = name[:-1]\n",
    "            out[attrib_name] = handle_special_attributes(attrib_name, json_to_flatten)\n",
    "\n",
    "    flatten(nested_json)\n",
    "    return out\n",
    "\n",
    "\n",
    "def filter_for_patient(entry):\n",
    "    return entry['resource']['resourceType'] == \"Patient\"\n",
    "\n",
    "\n",
    "def find_patient(bundle):\n",
    "    patients = list(filter(filter_for_patient, bundle['entry']))\n",
    "    if len(patients) < 1:\n",
    "        raise Exception('No Patient found in bundle!')\n",
    "    else:\n",
    "        patient = patients[0]['resource']\n",
    "\n",
    "        patient_id = patient['id']\n",
    "        first_name = patient['name'][0]['given'][0]\n",
    "        last_name = patient['name'][0]['family']\n",
    "\n",
    "        return {'PatientFirstName': first_name, 'PatientLastName': last_name, 'PatientID': patient_id}\n",
    "\n",
    "\n",
    "def flat_to_string(flat_entry):\n",
    "    output = ''\n",
    "\n",
    "    for attrib in flat_entry:\n",
    "        output += f'{attrib} is {flat_entry[attrib]}. '\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def flatten_bundle(bundle_file_name):\n",
    "    file_name = bundle_file_name[bundle_file_name.rindex('/') + 1:bundle_file_name.rindex('.')]\n",
    "    with open(bundle_file_name) as raw:\n",
    "        bundle = json.load(raw)\n",
    "        patient = find_patient(bundle)\n",
    "        flat_patient = flatten_fhir(patient)\n",
    "        for i, entry in enumerate(bundle['entry']):\n",
    "            flat_entry = flatten_fhir(entry['resource'])\n",
    "            with open(f'{flat_file_path}/{file_name}_{i}.txt', 'w') as out_file:\n",
    "                out_file.write(f'{flat_to_string(flat_patient)}\\n{flat_to_string(flat_entry)}')\n",
    "\n",
    "\n",
    "if not os.path.exists(flat_file_path):\n",
    "    os.mkdir(flat_file_path)\n",
    "\n",
    "for file in glob.glob(in_file_glob):\n",
    "    flatten_bundle(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9af9519682334df",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Setup the Gen AI with RAG\n",
    "\n",
    "This section will use LlamaIndex to construct the vector store and tie to the LLM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install llama-index\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad9e789f424c411",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "I tried a couple of different models for doing the embedding, i.e. turning the flattened FHIR text into vectors. I would like to experement with others, but haven't had time. In the end, `BAAI/bge-large-en-v1.5` was too big for me to run on my local, so I did most of my testing with `BAAI/bge-small-en-v1.5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a47aefc7f6f832e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from llama_index.embeddings import HuggingFaceEmbedding\n",
    "\n",
    "# loads BAAI/bge-small-en\n",
    "# embed_model = HuggingFaceEmbedding()\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "# embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-large-en-v1.5\")\n",
    "\n",
    "# embed_model = HuggingFaceEmbedding(model_name=\"medicalai/ClinicalBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cf86c33ac94bbd",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This code is to play with embedding if desired. It is not needed and can remain commented out.\n",
    "\n",
    "# embeddings = embed_model.get_text_embedding(\"Hello World!\")\n",
    "# print(len(embeddings))\n",
    "# print(embeddings[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6328dbe457d3dcde",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from llama_index.llms import Ollama\n",
    "\n",
    "# LLama 2 is running locally, using Ollama.\n",
    "llm = Ollama(model=\"llama2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7523591bb7324a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is a test prompt, just to prove that Ollama is working. It can remain commented out.\n",
    "\n",
    "# resp = llm.complete(\"Who is Paul Graham?\")\n",
    "# print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792169943ccb715",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from llama_index import ServiceContext, VectorStoreIndex, SimpleDirectoryReader, SummaryIndex\n",
    "from llama_index import set_global_service_context\n",
    "\n",
    "service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model)\n",
    "set_global_service_context(service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1067bc55158f53b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This code loads the flat FHIR text files. \n",
    "\n",
    "documents = SimpleDirectoryReader(flat_file_path).load_data()\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10086bb49ca618b8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load those flat FHIR text files into the vector store.\n",
    "\n",
    "vector_index = VectorStoreIndex.from_documents(documents, show_progress=True)\n",
    "\n",
    "\n",
    "# if not os.path.exists(vector_store_file_path):\n",
    "#     os.mkdir(vector_store_file_path)\n",
    "# vector_index.vector_store.persist(f'{vector_store_file_path}/FHIR_RAG.vs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcde194f8d979e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# I tried to play with summary indexes, but it took too long to get a response on my machine. \n",
    "\n",
    "# summary_index = SummaryIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5a5f3a941b5afa",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from llama_index.response.notebook_utils import display_response\n",
    "import logging\n",
    "import sys\n",
    "from IPython.core.display import Markdown\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9003e241adaae4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Actually do RAG\n",
    "\n",
    "This is the code block that actually asks the questions of the LLM. \n",
    "\n",
    "In my tests, I used synthetic FHIR generated by [Synthea](https://github.com/synthetichealth/synthea/wiki/Basic-Setup-and-Running). I had Synthea generate two patients and so I asked questions about each patient. If you are looking to replicate this work, you will need to change the patient names to match what ever data you have available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea468385e9d1c52a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def display_source_text(response):\n",
    "    for ind, source_node in enumerate(response.source_nodes):\n",
    "        display(Markdown(\"---\"))\n",
    "        display(Markdown(f\"**`Source Node {ind + 1}/{len(response.source_nodes)}`**\"))\n",
    "        text_md = (\n",
    "            f'**File:** {source_node.node.metadata[\"file_name\"]}<br>'\n",
    "            f'**Text:** {source_node.node.get_content().strip()}'\n",
    "        )\n",
    "        display(Markdown(text_md))\n",
    "\n",
    "\n",
    "def ask_question(index, response_mode, question, show_sources=False):\n",
    "    query_engine = index.as_query_engine(response_mode=response_mode, similarity_top_k=5)\n",
    "    response = query_engine.query(question)\n",
    "    display(Markdown(f'### Answer for {response_mode}'))\n",
    "    if show_sources:\n",
    "        display_source_text(response)\n",
    "    else:\n",
    "        display_response(response, show_source=False, show_metadata=False, show_source_metadata=False)\n",
    "\n",
    "\n",
    "def ask_question_all_modes(person, question):\n",
    "    display(Markdown(f'# Asking about {person}\\n<br>**Question:** {question}'))\n",
    "    ask_question(vector_index, 'no_text', question, show_sources=True)\n",
    "    ask_question(vector_index, 'simple_summarize', question)\n",
    "    ask_question(vector_index, 'compact', question)\n",
    "    ask_question(vector_index, 'refine', question)\n",
    "    ask_question(vector_index, 'tree_summarize', question)\n",
    "    ask_question(vector_index, 'accumulate', question)\n",
    "    ask_question(vector_index, 'compact_accumulate', question)\n",
    "\n",
    "\n",
    "ask_question_all_modes('Arnold',\n",
    "                       'What can you tell me about Arnold338 Wilkinson796 heart? For example, does he have hypertension?')\n",
    "ask_question_all_modes('Ashley', 'What can you tell me about Ashley34 Bergstrom287 allergies?')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
